---
title: "Lista 2"
author: "Pedro Henrique Corrêa de Almeida"
date: '2022-06-09'
output: html_document
---

# Questão 1

## Letra a

considerando $U_1$ e $U_2$ seguindo uma $U(0,1)$ independentes, vamos definir:

$$R = \sqrt{-2log(U_1)}$$

Pela função de distribuição de R, temos:

$$F_R(r) = P(R <= r)$$

Ou seja:

$$F_R(r) = P[\sqrt{-2log(U_1)} <= r]$$

Elevando ambos os lados da inequação ao quadrado:

$$F_R(r) = P[-2log(U_1) <= r^2]$$

Divididindo ambos os lados o $-2$, e, consequentemente invertendo o sinal da inequação:

$$F_R(r) = P[log(U_1) > \frac{-r^2}{2}]$$

Aplicando $exp(.)$ em ambos os lados:

$$F_R(r) = P(U_1 > e^{\frac{-r^2}{2}})$$

Dessa forma:

$$F_R(r) = 1-P(U_1 <= e^{\frac{-r^2}{2}})$$

Como, $U_1$ ~ $U(0,1)$, pela sua função de distribuição, sabemos que $P(U_1<=u) = u$, logo, considerando $u= e^{\frac{-r^2}{2}}$, temos:

$$F_R(r) = 1-F_{U_1}(e^{\frac{-r^2}{2}})$$

$$F_R(r) = 1-e^{\frac{-r^2}{2}}$$
Derivando em relação a $r$, aplicando a regra da cadeia, temos que a função de densidade de R:

$$f_R(r) = r\ e^{\frac{-r^2}{2}}$$
Definindo:

$$\Theta = 2 \pi U_2$$

Pela função de distribuição de $\Theta$:

$$F_\Theta(\theta)=P(\Theta<=\theta)$$
$$F_\Theta(\theta)=P(2 \pi U_2<=\theta)$$

Dividindo ambos lados por $2 \pi$, temos:

$$F_\Theta(\theta)=P(U_2<=\frac{\theta}{2 \pi})$$
Como, $U_2$ ~ $U(0,1)$, pela sua função de distribuição, sabemos que $P(U_2<=u) = u$, logo, considerando $u= \frac{\theta}{2 \pi}$, temos:

$$F_\Theta(\theta) = F_{U_2}(\frac{\theta}{2 \pi})$$

$$F_\Theta(\theta) = \frac{\theta}{2 \pi}$$

Derivando em relação a $\theta$, temos que a função de densidade de V:

$$f_\Theta(\theta) = \frac{1}{2 \pi}$$

Dessa forma, definimos:

$$X_1 = R\ cos(\Theta)$$

$$X_2 = R\ sen(\Theta)$$
Ou seja, podemos expressar:

$$X_1 = g_1(R,\Theta)$$
$$X_2 = g_2(R, \Theta)$$

E, inversamente:

$$R = h_1(X_1, X_2) = \sqrt{X_1^2+X_2^2}$$

$$\Theta = h_2(X_1, X_2) = arctg(\frac{X_1}{X_2})$$

Pelo método jacobiano, definimos:

$$f_{X_1, X_2}(x_1, x_2) = f_{R,\Theta}(h_1(x_1, x_2), h_2(x_1, x_2)) \ \|J\|$$

Como, $U_1$ e $U_2$ são independentes, temos que R e $\Theta$ também são, logo a conjunta de R e $\Theta$:

$$f_{R, \Theta}(r,\theta)=f_R(r)f_\Theta(\theta)=\frac{r}{2 \pi} e^{\frac{-r^2}{2}}$$

ou seja:

$$f_{R,\Theta}(h_1(x_1, x_2), h_2(x_1, x_2))=\frac{\sqrt{x_1^2+x_2^2}}{2 \pi} e^{\frac{-(x_1^2+x_2^2)}{2}}$$

Além disso, definimos o jacobiano:

$$J = 
\begin{bmatrix}
    \frac{\partial{h1}}{\partial{x_1}}       & \frac{\partial{h1}}{\partial{x_2}}\\
    \frac{\partial{h2}}{\partial{x_1}}       & \frac{\partial{h2}}{\partial{x_2}} \\
\end{bmatrix} =
\begin{bmatrix}
    \frac{-2x_1}{2\sqrt{x_1^2+x_2^2}}       & \frac{-2x_2}{2\sqrt{x_1^2+x_2^2}}\\
    \frac{1}{x_2\ (\frac{x_1^2}{x_2^2}+1)}       & \frac{-x_1}{x_2^2\ (\frac{x_1^2}{x_2^2}+1)} \\
\end{bmatrix}
$$
Dessa forma, temos que:

$$\|J\| = \frac{1}{\sqrt{x_1^2+x_2^2}}$$

Com isso:

$$f_{X_1, X_2}(x_1, x_2) = \frac{\sqrt{x_1^2+x_2^2}}{2 \pi} e^{\frac{-(x_1^2+x_2^2)}{2}} \frac{1}{\sqrt{x1^2+x2^2}}$$

Logo:

$$f_{X_1, X_2}(x_1, x_2) = \frac{1}{2 \pi} e^{\frac{-(x_1^2+x_2^2)}{2}}$$
Pela propriedade do produto da exponencial de exponencial, temos:

$$f_{X_1, X_2}(x_1, x_2) = \frac{1}{2 \pi} e^{\frac{-x_1^2}{2}} \ e^{\frac{-x_2^2}{2}}$$
Uma vez que, $2\pi = \sqrt{2\pi}\sqrt{2\pi}$, concluímos:

$$f_{X_1, X_2}(x_1, x_2) = \frac{1}{\sqrt{2\pi}} e^{\frac{-x_1^2}{2}} \ \frac{1}{\sqrt{2\pi}}e^{\frac{-x_2^2}{2}}$$

Dessa forma, consideramos $X_1$ e $X_2$ independentes, logo:

$$f_{X_1, X_2}(x_1, x_2) = f_{X_1}(x_1)f_{X_2}(x_2)$$

Ou seja:

$$f_{X_1}(x_1) = \frac{1}{\sqrt{2\pi}} e^{\frac{-x_1^2}{2}}$$
$$f_{X_2}(x_2) = \frac{1}{\sqrt{2\pi}} e^{\frac{-x_2^2}{2}}$$

Ou seja, pela função de densidade de $X_1$ e $X_2$, temos que ambas as variáveis aleórias seguem $N(0,1)$

## Letra b

### i

Considerando $W = R^2$, temos a função de distribição de $W$:

$$F_{W}(w) = P(W<=w) = P(R^2 <= w) = P(R <= \sqrt{w}) = F_R(\sqrt{w})$$
Como foi encontrado na letra a, temos que a função de distribuição de $R$ é:

$$F_R(r) = 1 - e^{\frac{-r^2}{2}}$$

Ou seja:

$$F_{W}(w) = F_R(\sqrt{w}) = 1 - e^{\frac{-w}{2}}$$
Logo, pela função de distribuição de $W=R^2$, temos que segue $Exp(\frac{1}{2})$

### ii

Como foi encontrado na letra a, consideramos:

$$R = \sqrt{-2log(U_1)}$$
$$\Theta = 2 \pi U_2$$
E encontramos que, $X_1$ e $X_2$ seguem $N(0,1)$, onde:

$$X_1 = R \ cos(\Theta)$$
$$X_2 = R \ sen(\Theta)$$

Ou seja:

$$\frac{sen(\Theta)}{cos(\Theta)} = \frac{X_2}{X_1}$$

$$tan(\Theta) = \frac{X_2}{X_1}$$
Aplicando a função $arctan$ em ambos os lados da igualdade:

$$\Theta = arctan(\frac{X_2}{X_1})$$

Como foi encontrado na letra a, sabemos que $f_\Theta(\theta)$ é dada por:

$$f_\Theta(\theta) = \frac{1}{2 \pi}$$

Ou seja, temos que $\Theta$ ~ $U(0,2\pi)$, uma vez que a densidade de uma uniforme $f_U(u) = \frac{1}{b-a}$.

## Letra c

```{r}
gera_normal_box_muller <- function(N = 1E4){
  
  U1 <- runif(N)
  U2 <- runif(N)
  
  d <- -2*log(U1)
  teta <- 2*pi*U2
  
  Z1 <- sqrt(d)*cos(teta)
  Z2 <- sqrt(d)*sin(teta)
  
  list(Z1, Z2)
}
```

## Letra d

```{r}
# r_2 <- function(x) exp(x^2/2)
# curve(r_2, from = 0, to = 1)
```


```{r}
gera_r2 <- function(n){
  
  U <- runif(n)
  R2 <- -2*log(U)
  R2
}
```

```{r}
hist(gera_r2(1000), freq = F)
```

```{r}
gera_theta <- function(n){
  
  U <- runif(n)
  theta <- 2*pi*U
  theta
}
```

```{r}
hist(gera_theta(1000), freq = F)
```

## Letra e

```{r}
gera_normal_box_muller_polar <- function(N = 1E4){

  faltantes <- N
  X1 <- c()
  X2 <- c()
  
  while(faltantes > 0){
    U1 <- runif(faltantes)
    U2 <- runif(faltantes)
  
    V1 <- 2*U1 - 1
    V2 <- 2*U2 - 1
    S <- V1**2 + V2**2
    
    S <- ifelse(S < 1, S, NA)
    na_i <- is.na(S)
    faltantes <- sum(na_i)
    S <- S[!na_i]
    
    Z <- sqrt(-2*log(S)/S)
    
    X1_i <- Z * V1[!na_i]
    X2_i <- Z * V2[!na_i]

    X1 <- c(X1, X1_i)
    X2 <- c(X2, X2_i)
  }
  list(X1, X2)
}
```

## Letra f

```{r}
gera_normal_exp <- function(N = 1E4){

  faltantes <- N
  X <- c()
  
  while(faltantes > 0){
    Y1 <- rexp(faltantes)
    Y2 <- rexp(faltantes)
    
    Y <- ifelse(Y2 > (1-Y1**2)/2, Y1, NA)
    na_i <- is.na(Y)
    faltantes <- sum(na_i)
    
    Y <- Y[!na_i]
    U <- runif(length(Y))
    Xi <- ifelse(U < 0.5, Y, -Y)

    X <- c(X, Xi)
  }
  X
}
```

```{r}
hist(gera_normal_exp(10000))
```

```{r}
library(microbenchmark)
N <- 1E4
microbenchmark(unlist(gera_normal_box_muller(N/2)), 
               unlist(gera_normal_box_muller_polar(N/2)),
               gera_normal_exp(N))
```


## Letra g

```{r}
gera_normal_tcl_unif <- function(n, n_uniforme){
  
  U <- replicate(n, runif(n_uniforme)-0.5)
  Z <- apply(U, 2, sum)
  Z/sqrt(n_uniforme/12)
}
```



### i

Temos $Z = \sum^n_{i=1}U_i$, com $U_i$ ~ $U(-\frac{1}{2},\frac{1}{2})$ logo, como a esperança da soma é a soma das esperanças temos:

$$E(Z) = E(\sum^n_{i=1}U_i) = \sum^n_{i=1}E(U_i)$$

Como $U_i$ são identicamente distribuídas, onde:

$$E(U_i) = \frac{a+b}{2} = \frac{\frac{-1}{2} + \frac{1}{2}}{2} = 0$$
Logo o somatório das esperanças de $U_i$ será zero, ou seja:

$$E(Z) = 0$$

Além disso, temos a variância de $Z$ como:

$$Var(Z) = Var(\sum^n_{i=1}U_i)$$

Como $U_i$ são independentes, temos que a variância da soma vai ser a soma das variâncias:

$$Var(Z) = \sum^n_{i=1}Var(U_i)$$

Sabemos que a variância de uma uniforme, é dada por:

$$Var(U) = \frac{1}{12}(b-a)^2$$

Considerando $a=-\frac{1}{2}$ e $b = \frac{1}{2}$, temos:

$$Var(U_i) = \frac{1}{12}$$
Como $U_i$ são identicamente distribuídas, temos:

$$Var(Z) = n \ Var(U_1) = \frac{n}{12}$$

```{r}
n <- c(12, 48, 96)
amostras_tcl <- lapply(n, function(n) gera_normal_tcl_unif(1000, n))
```


```{r}
par(mfrow = c(2, 2))
lapply(1:3,
       function(i){
          hist(amostras_tcl[[i]], freq = F, main = paste("n =", n[i]), breaks = 20)
         }
       )
```



```{r}
lapply(c(12, 48, 96),
       function(n){
          summary(replicate(1000, shapiro.test(gera_normal_tcl_unif(100, n))$p.value))
         }
       )
```

```{r}
N <- 1E4
microbenchmark(unlist(gera_normal_box_muller(N/2)), 
               unlist(gera_normal_box_muller_polar(N/2)),
               gera_normal_exp(N),
               gera_normal_tcl_unif(N, 12))
```

## Letra h

Considerando, a aproximação da inversa de uma função de distribuição da normal padrão:

$$\Phi^{-1}(alpha) = t-\frac{a_0+a_1t}{1+b_1t+b_2t^2}\\
\ t=log(\alpha^{-2}), \ a_0=2,30753, \ a_1 = 0,27061, \ b_1 = 0,99229, \ b_2 = 0,04481$$

```{r}
 # Definindo as constantes
a_0 <- 2.30753; a_1 <- 0.27061;b_1 <- 0.99229; b_2 <- 0.04481

inversa_phi <- function(alpha){

  t2 <- ifelse(alpha == 0, 
               -Inf, 
               ifelse(alpha == 1, 
                      Inf, 
                      -2*log(alpha)))
  t1 <- sqrt(t2)

  x <- ifelse(!is.finite(t2), 
              t2, 
              -(t1 - ((a_0+a_1*t1)/(1+b_1*t1+b_2*t2))))
  x
}
a <- seq(0, 1, 0.05)
inversa_phi(a)
qnorm(a)
```

```{r}
gera_normal_as <- function(n){
  U <- runif(n)
  inversa_phi(U)
}
```

```{r}
x_normal_as <- gera_normal_as(5E3)
```

```{r}
hist(x_normal_as, freq = F)
```

```{r}
N <- 1E4
microbenchmark(unlist(gera_normal_box_muller(N/2)), 
               gera_normal_as(N))
```


```{r}
summary(replicate(1000, shapiro.test(unlist(gera_normal_box_muller(1E3/2)))$p.value))
```


```{r}
summary(replicate(1000, shapiro.test(unlist(gera_normal_as(1E3/2)))$p.value))
```


```{r}

```


# Questão 2

## Letra a

Considerando, $X_1$ e $X_2$ independentes seguindo $N(0,1)$, definimos:

$$Y = \frac{X_1}{X_2}$$

$$W=X_2$$

Podemos definir:

$$X_1 = YW$$
$$X_2 = W$$
Definindo o módulo do jacobiano, temos

$$J = 
\begin{bmatrix}
    W       & Y\\
    0       & 1 \\
\end{bmatrix} = W
$$
Ou seja:

$$|J| = |W|$$
Como $X_1$ e $X_2$ são independentes, temos a conjunta:

$$f_{X_1,X_2}(x_1, x_2) = \frac{1}{2\pi}e^{\frac{-(x_1^2+x_2^2)}{2}}$$

Dessa forma:

$$f_{Y,W}(y, w) = f_{X_1,X_2}(yw, w) |J| = \frac{1}{2\pi}e^{\frac{-(y^2w^2+w^2)}{2}} \ |w|$$
 A fim de encontrar $f_Y(y)$, vamos integrar $f_{Y,W}(y, w)$, em relação a w:

$$f_Y(y) = \frac{1}{2\pi}\int_{-\infty}^\infty e^{\frac{-(y^2w^2+w^2)}{2}} \ |w| dw$$
Como $W$ é simétrica, podemos integrar apenas no intervalo 0 a $\infty$ e multiplicar por dois, dessa forma podemos considerar $|w|=w$

$$f_Y(y) = \frac{2}{2\pi}\int_{0}^\infty e^{\frac{-(y^2w^2+w^2)}{2}} \ w\  dw$$
Considerando $t=\frac{w^2(y^2+1)}{2}$, logo $dt = w(y^2+1) \ dw$, ou seja $w \ dw= \frac{dt}{y^2+1}$:

$$f_Y(y) = \frac{1}{\pi}\int_{0}^\infty e^{-t} \frac{dt}{y^2+1} = \frac{1}{\pi (y^2+1)}$$
Logo, temos que $Y$~$Cauchy(0,1)$

```{r}
gera_cauchy_box_muller <- function(n){
  X <- gera_normal_box_muller(n)
  X[[1]]/X[[2]]
}
```

```{r}
hist(gera_cauchy_box_muller(1000), breaks = 50)
```

## Letra b

Sabemos que se $X$ ~ $Cauchy(0, 1)$, sua função de densidade é dada por:

$$f_X(x) = \frac{1}{\pi (y^2+1)}$$
A fim, de encontrar a função de distribuição de $X$ vamos integrar $f_x(x)$ a fim de encontrar sua primitiva:

$$F_X(x) = \int\frac{1}{\pi (x^2+1)} \ dx$$
$$F_X(x) = \frac{1}{\pi} \int\frac{1}{(x^2+1)} \ dx$$
Como a derivada do $arctan(x)$ é $\frac{1}{x^2+1}$, concluímos:

$$F_X(x) = \frac{arctan(x)}{\pi}$$

Dessa forma, temos a inversa aplicada na função de distribuição como:

$$F_X(F_X^{-1}(p)) = p$$

$$\frac{arctan(F_X^{-1}(p))}{\pi} = p$$
Multiplicando ambos os lados por $\pi$ e aplicando a função tangente:

$$F_X^{-1}(p) = tan(\pi p)$$

```{r}
inversa_cauchy <- function(p){
  sapply(p, function(pj) tan(pi*pj))
}
gera_cauchy_inversa <- function(n){
  U <- runif(n)
  inversa_cauchy(U)
}
```

```{r}
hist(gera_cauchy_inversa(1000))
```

## Letra c

```{r}
library(microbenchmark)
```

```{r}
N <- 1E4
microbenchmark(gera_cauchy_box_muller(N), 
               gera_cauchy_inversa(N))
```

# Questão 3

```{r}
gera_exp_cond <- function(n){
  
  faltantes <- n
  X <- c()
  
  while(faltantes > 0){
    xi <- rexp(faltantes)
    xi <- ifelse(xi<0.05, xi, NA)
    na_xi <- is.na(xi)
    X <- c(X, xi[!na_xi])
    faltantes <- sum(na_xi)
  }
  X
}
```

```{r}
f_3 <- function(x) exp(-x)/(1-exp(-0.05))
x_3 <- gera_exp_cond(1000)
hist(x_3, freq=F, breaks = 30)
curve(f_3, from = 0, to = 0.05, add = T)
```

```{r}
mean(gera_exp_cond(10000))
```

## Letra b

Dado que:

$$
f_X(x)=
\begin{cases}
\frac{e^{-x}}{1-e^{-0,05}}, se\ 0<x<0,05\\
0,\ caso\ contrário\\
\end{cases}
$$
Temos que $E(X|X<0,05)$ é dada pela integral de $\int{xf_X(x)dx}$, ou seja:

$$E(X|X<0,05) = \int_{-\infty}^0{x \ 0 \ dx}+\int_0^{0,05}{\frac{x \ e^{-x}}{1-e^{-0,05}} \ dx}+\int_{0,05}^\infty{x \ 0 \ dx}$$
$$E(X|X<0,05) = \int_0^{0,05}{\frac{x \ e^{-x}}{1-e^{-0,05}} \ dx}$$

$$E(X|X<0,05) = \frac{1}{1-e^{-0,05}}\int_0^{0,05}{x \ e^{-x} \ dx}$$

Utilizando derivação por partes, onde:

$$u = x\ \ \ \ \ \ \ \ \  dv = e^{-x}$$
$$du = dx\ \ \ \ \ \ \ \ \  v = -e^{-x}$$

Temos que:

$$\int_0^{0,05}{x \ e^{-x} \ dx} = [-x \ e^{-x}]_0^{0,05} -\int_0^{0,05}{-e^{-x} \ dx}$$

$$\int_0^{0,05}{x \ e^{-x} \ dx} = [-x \ e^{-x}]_0^{0,05} + [-e^{-x}]_0^{0,05}$$

$$\int_0^{0,05}{x \ e^{-x} \ dx} = [-x \ e^{-x} -e^{-x}]_0^{0,05} = [-e^{-x}(x+1)]_0^{0,05}$$

$$\int_0^{0,05}{x \ e^{-x} \ dx} = -e^{-0,05}(1,05)+e^0$$
Logo:

$$E(X|X<0,05) = \frac{1}{1-e^{-0,05}}[-e^{-0,05}(1,05)+e^0] = 0.02479168$$

# Questão 4

Uma vez que temos:

$$
F_X(x)=
\begin{cases}
\frac{1-e^{-2x}+2x}{3}, se\ 0<x<1\\
\frac{3-e^{-2x}}{3}, se\ 1<x<\infty\\
\end{cases}
$$
Ou seja, podemos considerar uma mistura de duas distribuições, onde:

$$F_X(x)=p_1F_{X_1}(x_1)+p_2F_{X_2}(x_2)$$

Em que:

$$F_{X_1}(x_1)=\frac{1-e^{-2x_1}+2x_1}{3}, 0<x_1<1\\
F_{X_2}(x_2)\frac{3-e^{-2x_2}}{3}, 1<x_2<\infty\\$$

Afim de achar $p_1$, vamos calcular a probabilidade de X estar entre 0 e 1:

$$p_1 = F_{X_1}(1)-F_{X_1}(0) = \frac{1-e^{-2}+2}{3} - \frac{1-e^{0}}{3} = \frac{3-e^{-2}}{3}-0=0.9548882$$
Além disso, temos $p_2$ como:

$$p_2 = 1-F_{X_2}(1) = 1 - \frac{3-e^{-2}}{3} = 1-0.9548882=0.04511176$$
Dessa forma, estabelecemos que $p_1+p_2=1$.

A fim de contruir um gerador de números aleatórios dessa distribuição, vamos utiliizar o método da inversa. Para $F_{X_2}(x_2)$ podemos achar sua inversa analiticamente da seguinte forma:

$$F_{X_2}(F_{X_2}^{-1}(p)) = p\\
\frac{3-e^{-2F_{X_2}^{-1}(p)}}{3} = p\\
e^{-2F_{X_2}^{-1}(p)} = 3-3p$$

Aplicando log em ambas as partes, concluímos:

$$-2F_{X_2}^{-1}(p) = log(3-3p)\\
F_{X_2}^{-1}(p) = -\frac{log(3-3p)}{2}$$

No caso de $F_{X_1}(x_1)$ vamos utiliizar o método da inversa através da funçãso `optimize` para achar a raiz da equação:

$$(F_X(F_X^{-1}(p)) - p)^2 = 0$$
$$(F_X(x) - p)^2 = 0$$

E como, $0<x_1<1$, vamos utilizar o intervalo $[0,1]$ a fim de achar essa raiz.

Além disso, como vamos gerar uma uniforme entre 0 e 1 a fim de gerar os valores para p, se o valor gerado for menor que $p_1$, ou seja, x estará entre 0 e 1, vamos utilizar $F_{X_1}(x_1)$, caso contrário, vamos utilizar $F_{X_2}(x_2)$:

```{r}
FX1 <- function(x) (1-exp(-2*x)+2*x)/3
inversa_FX1 <- function(p) optimize(function(x) (FX1(x) - p)**2, interval = c(0, 1), tol = 1E-6)$minimum

FX2 <- function(x) (3-exp(-2*x))/3
inversa_FX2 <- function(p) (-log(3-3*p))/2

gera_mistura <- function(n, inversa_FX1, inversa_FX2){
  
  p1 <- (3-exp(-2))/3
  
  U <- runif(n)
  ifelse(U<p1,
         sapply(U, function(pj) inversa_FX1(pj)),
         sapply(U, function(pj) inversa_FX2(pj)))
  
}
```

```{r}
n <- 1000
x_mistura <- gera_mistura(n, inversa_FX1, inversa_FX2)
```

```{r}
hist(x_mistura, freq=F)
```

```{r}
plot(ecdf(x_mistura), lwd = 3)
curve(FX1, from = 0, to = 1, col = "blue", add = T, lwd = 2)
curve(FX2, from = 1, to = 5, col = "red", add = T, lwd = 2)
```


# Questão 5

```{r}
gera_normal_multi_matriz_A <- function(n, dim, mu, sigma){
  
  Z <- replicate(dim, unlist(gera_normal_box_muller(n/2)))
  A <- t(chol(sigma))
  X <- lapply(1:n, function(i){
    t(A%*%Z[i,]+mu)
    })
  X <- do.call(rbind, X)
  return(X)
}
```

```{r}
mu <- c(1, 3)
ro <- -0.8
sigma <- matrix(c(2, ro*sqrt(2), ro*sqrt(2), 1), ncol = 2)
sigma
```

```{r}
x_multivariada <- gera_normal_multi_matriz_A(5E3, 2, mu, sigma)
```

```{r}
calcula_ellipse <- function(confidence, mu, sigma){
  auto <- eigen(sigma)
  
  max_auto_value <- which(auto$values == max(auto$values))
  max_auto_vet <- auto$vectors[,max_auto_value]
  angulo <- atan2(max_auto_vet[2], max_auto_vet[1])
  if(angulo < 0){
    # mudar para intervalo [0, 2pi]
    angulo <- angulo + 2*pi
  }
  
  theta <- seq(0, 2*pi, length.out = 100)
  phi <- angulo
  
  a <- sqrt(qchisq(confidence, 2)*auto$values[max_auto_value])
  b <- sqrt(qchisq(confidence, 2)*auto$values[-max_auto_value])
  
  elipse_x <- a*cos(theta)
  elipse_y <- b*sin(theta)
  
  R <- matrix(c(cos(phi), sin(phi), -sin(phi), cos(phi)), ncol = 2, byrow = T)
  r_ellipse <- matrix(c(elipse_x, elipse_y), ncol = 2) %*% R
  
  pontos <- r_ellipse + matrix(rep(mu, dim(r_ellipse)[1]), ncol = 2, byrow = T)
  
  return(list(pontos = pontos, a = a, b = b, phi = phi))
  }
```

```{r}
pontos_fora <- function(x, y, a, b, phi, mu){
  (((x-mu[1])*cos(phi)+(y-mu[2])*sin(phi))**2/a**2)+(((x-mu[1])*sin(phi)-(y-mu[2])*cos(phi))**2/b**2) > 1
}
```

```{r}
ellipse <- calcula_ellipse(0.95, mu, sigma)
pontos <- ellipse$pontos
```

```{r}
fora <- pontos_fora(X[, 1], X[, 2], ellipse$a, ellipse$b, ellipse$phi, mu)
```

```{r}
plota_ellipse <- function(x, confidence, mu, sigma){
  
  ellipse <- calcula_ellipse(confidence, mu, sigma)
  pontos <- ellipse$pontos
  fora <- pontos_fora(x[, 1], x[, 2], ellipse$a, ellipse$b, ellipse$phi, mu)
  
  plot(x[fora, 1], x[fora, 2], col = "red", bty = "n", main = sprintf("Elipse da região de %.02f%% de confiança", confidence*100))
  points(x[!fora, 1], x[!fora, 2])
  points(mu[1], mu[2], pch = 20, col = "blue")
  lines(pontos[, 1], pontos[, 2], col = "blue", lwd = 2)
  legend("topright", legend = sprintf("%s pontos(%s%%) \nfora da elipse", sum(fora), round(sum(fora)*100/(length(x)/2), 2)), bty = "n")
  
}
```



```{r}
layout(matrix(c(1,1,2,2,0,3,3,0), ncol=4, byrow = T))
invisible(
  lapply(c(0.68,0.95,0.99),
         function(conf)
           plota_ellipse(x_multivariada, conf, mu, sigma)
         )
  )
```









